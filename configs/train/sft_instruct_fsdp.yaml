model: <<model_name>> # Path to the the Huggingface model
ptmp_dir: <<ptmp_dir>> # Set to /ptmp/$USER to save the fine-tuned model in the ptmp directory (ptmp_dir + /output_dir). If None, the model will be saved in the output_dir.
train_args: # Check the Huggingface Trainer documentation for more details
  output_dir: <<output_dir>>/models
  # removed_unused_columns = False prevents a bug with training PEFT models,
  # since remove_unused_columns works by inspecting the signature of the
  # model forward call, and this doesn't work properly for PeftModel.
  seed: 4242
  learning_rate: "<<float: lr>>"
  warmup_ratio: 0.05 # Avoiding that the model is trained with a too high learning rate at the beginning
  weight_decay: 0.01 # Pytorch default
  num_train_epochs: "<<int: num_train_epochs>>"
  per_device_train_batch_size: "<<int: per_device_train_batch_size>>"
  per_device_eval_batch_size: "<<int: per_device_eval_batch_size>>"
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: False
  bf16: true
  save_strategy: epoch
  eval_strategy: "<<eval_strategy|steps>>"
  logging_steps: 1
  eval_steps: 0.1
  log_on_each_node: false
  logging_first_step: true
  eval_on_start: true
  report_to: [] # change to wandb once I have access to wandb
  optim: paged_adamw_32bit
  gradient_checkpointing: true
  lr_scheduler_type: linear # Linear decay of the learning rate, looks like resulting in more smooth performance towards the end of training
  run_name: <<name>> | <<job_id>> | <<test_fold>>
  max_steps: "<<int: max_steps|-1>>"
train_dataset_config:
  data_path: projects/collectively_grounded_llms/data/processed/toy_dataset
  parser_config:
    fields:
      system_prompt:
        text: "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are conversation generator. You will be given a player and an NPC with their personas. Your task is to generate a conversation between the player and the NPC.<|eot_id|>\n<|start_header_id|>user<|end_header_id|>"
      user_prompt:
        text: "\nHere is the information about the NPC and the player:\nNPC: {npc_name}\nPersona: {npc_persona}\n\nPlayer: {player_name}\nPersona: {player_persona}\n\nCan you create a conversation?\n<|start_header_id|>assistant<|end_header_id|>"
      conversation:
        text: "\n{conversation}<|eot_id|>"
