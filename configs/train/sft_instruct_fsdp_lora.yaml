model: <<model_name>> # Path to the the Huggingface model
ptmp_dir: <<ptmp_dir>> # Set to /ptmp/$USER to save the fine-tuned model in the ptmp directory (ptmp_dir + /output_dir). If None, the model will be saved in the output_dir.
train_args: # Check the Huggingface Trainer documentation for more details
  output_dir: <<output_dir>>/models
  seed: 4242
  learning_rate: "<<float: lr>>"
  weight_decay: 0.0
  num_train_epochs: "<<int: num_train_epochs>>"
  per_device_train_batch_size: "<<int: per_device_train_batch_size>>"
  per_device_eval_batch_size: "<<int: per_device_eval_batch_size>>"
  # resume_from_checkpoint: "<<str: resume_from_checkpoint>>"
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: False
  bf16: true
  save_strategy: epoch
  eval_strategy: steps
  logging_steps: 1
  eval_steps: 20
  do_eval: true
  log_on_each_node: false
  logging_first_step: true
  report_to: [] # change to wandb once I have access to wandb
  optim: paged_adamw_32bit
  gradient_checkpointing: true
  lr_scheduler_type: constant
  run_name: <<name>> | <<job_id>> | <<test_fold>>
use_flash_attention: true 
peft_config: # Check the PEFT documentation for more details
  peft_type: LORA
  task_type: CAUSAL_LM
  lora_alpha: 16
  lora_dropout: 0.0
  r: 64
  bias: none
#logging_group_for_custom_eval: # Custom evaluation logging groups to log and aggregate the logp values for each group name.
# - name: first_round
#   filter:
#    rounds: [0]
# - name: end_game_messages  # Name of the logging group in Wandb
#   filter:
#    types: ["message"] # Filter by event type "message"
#    rounds: [25,26,27,28,29] # Filter by specific rounds
# - name: messages
#   filter: 
#    types: ["message"]
# - name: actions
#   filter: 
#    types: ["action"]
# - name: all
#   filter: {}
train_dataset_config:
  test_fold: <<test_fold>> # The fold to use for testing (default is 0)
  data_path: <<dataset>>  # Path to the dataset. Tip: Use "dev_replay_instruct" for debugging and fast training.
  parser_config:
    fields:
      system_prompt:
        text: "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are conversation generator. You will be given a player and an NPC with their personas. Your task is to generate a conversation between the player and the NPC.<|eot_id|>\n<|start_header_id|>user<|end_header_id|>"
      user_prompt:
        text: "\nHere is the information about the NPC and the player:\nNPC: {npc_name}\nPersona: {npc_persona}\n\nPlayer: {player_name}\nPersona: {player_persona}\n\nCan you create a conversation?\n<|start_header_id|>assistant<|end_header_id|>"
      conversation:
        text: "\n{conversation}<|eot_id|>"